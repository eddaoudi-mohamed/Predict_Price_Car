{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f792c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from typing import Tuple, Optional\n",
    "import logging\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0cb555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataSplitter:\n",
    "    \"\"\"\n",
    "    Advanced data splitting utility optimized for car price prediction\n",
    "    \n",
    "    Features:\n",
    "    - Multiple splitting strategies (random, temporal, stratified)\n",
    "    - Automatic target distribution preservation\n",
    "    - Memory-efficient chunked processing\n",
    "    - Configuration via YAML file\n",
    "    - Comprehensive logging\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize splitter with configuration\n",
    "        \n",
    "        Args:\n",
    "            config_path: Path to YAML configuration file\n",
    "        \"\"\"\n",
    "        self.config = self._load_config(config_path)\n",
    "        self._validate_config()\n",
    "        \n",
    "    def _load_config(self, config_path: Optional[str]) -> dict:\n",
    "        \"\"\"Load configuration from YAML file\"\"\"\n",
    "        default_config = {\n",
    "            'splitting': {\n",
    "                'test_size': 0.2,\n",
    "                'random_state': 42,\n",
    "                'stratify': False,\n",
    "                'temporal_split': False,\n",
    "                'time_column': None,\n",
    "                'chunksize': None\n",
    "            },\n",
    "            'paths': {\n",
    "                'input_data': 'data/processed/cleaned_data.csv',\n",
    "                'output_dir': 'data/splits/'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if config_path:\n",
    "            try:\n",
    "                with open(config_path) as f:\n",
    "                    user_config = yaml.safe_load(f)\n",
    "                return {**default_config, **user_config}\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load config: {e}. Using defaults\")\n",
    "                return default_config\n",
    "        return default_config\n",
    "    \n",
    "    def _validate_config(self) -> None:\n",
    "        \"\"\"Validate configuration parameters\"\"\"\n",
    "        if self.config['splitting']['temporal_split'] and not self.config['splitting']['time_column']:\n",
    "            raise ValueError(\"time_column must be specified for temporal split\")\n",
    "        if not 0 < self.config['splitting']['test_size'] < 1:\n",
    "            raise ValueError(\"test_size must be between 0 and 1\")\n",
    "    \n",
    "    def split_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "        \"\"\"\n",
    "        Execute the data splitting process\n",
    "        \n",
    "        Returns:\n",
    "            X_train, X_test, y_train, y_test DataFrames/Series\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        df = self._load_data()\n",
    "        \n",
    "        # Prepare features and target\n",
    "        X = df.drop(columns=[self.config['target_column']])\n",
    "        y = df[self.config['target_column']]\n",
    "        \n",
    "        # Select splitting strategy\n",
    "        if self.config['splitting']['temporal_split']:\n",
    "            return self._temporal_split(X, y)\n",
    "        else:\n",
    "            return self._random_split(X, y)\n",
    "    \n",
    "    def _load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load and validate input data\"\"\"\n",
    "        input_path = self.config['paths']['input_data']\n",
    "        chunksize = self.config['splitting']['chunksize']\n",
    "        \n",
    "        logger.info(f\"Loading data from {input_path}\")\n",
    "        \n",
    "        try:\n",
    "            if chunksize:\n",
    "                chunks = []\n",
    "                for chunk in pd.read_csv(input_path, chunksize=chunksize):\n",
    "                    chunks.append(chunk)\n",
    "                df = pd.concat(chunks, axis=0)\n",
    "            else:\n",
    "                df = pd.read_csv(input_path)\n",
    "                \n",
    "            # Validate required columns\n",
    "            required_cols = self.config.get('required_columns', [])\n",
    "            if required_cols and not all(col in df.columns for col in required_cols):\n",
    "                missing = set(required_cols) - set(df.columns)\n",
    "                raise ValueError(f\"Missing required columns: {missing}\")\n",
    "                \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load data: {e}\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    def _random_split(self, X: pd.DataFrame, y: pd.Series) -> Tuple:\n",
    "        \"\"\"Standard randomized train-test split\"\"\"\n",
    "        stratify = y if self.config['splitting']['stratify'] else None\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            test_size=self.config['splitting']['test_size'],\n",
    "            random_state=self.config['splitting']['random_state'],\n",
    "            stratify=stratify\n",
    "        )\n",
    "        \n",
    "        self._save_splits(X_train, X_test, y_train, y_test)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def _temporal_split(self, X: pd.DataFrame, y: pd.Series) -> Tuple:\n",
    "        \"\"\"Time-based splitting (chronological order)\"\"\"\n",
    "        time_col = self.config['splitting']['time_column']\n",
    "        cutoff = self.config['splitting'].get('cutoff_date')\n",
    "        \n",
    "        if not cutoff:\n",
    "            # Auto-determine cutoff based on test_size\n",
    "            sorted_dates = X[time_col].sort_values()\n",
    "            cutoff_idx = int(len(X) * (1 - self.config['splitting']['test_size']))\n",
    "            cutoff = sorted_dates.iloc[cutoff_idx]\n",
    "        \n",
    "        mask = X[time_col] <= cutoff\n",
    "        X_train, y_train = X[mask], y[mask]\n",
    "        X_test, y_test = X[~mask], y[~mask]\n",
    "        \n",
    "        self._save_splits(X_train, X_test, y_train, y_test)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def _save_splits(self, \n",
    "                    X_train: pd.DataFrame, \n",
    "                    X_test: pd.DataFrame,\n",
    "                    y_train: pd.Series,\n",
    "                    y_test: pd.Series) -> None:\n",
    "        \"\"\"Persist split data to disk\"\"\"\n",
    "        output_dir = Path(self.config['paths']['output_dir'])\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            X_train.to_csv(output_dir / 'X_train.csv', index=False)\n",
    "            X_test.to_csv(output_dir / 'X_test.csv', index=False)\n",
    "            y_train.to_csv(output_dir / 'y_train.csv', index=False)\n",
    "            y_test.to_csv(output_dir / 'y_test.csv', index=False)\n",
    "            logger.info(f\"Split data saved to {output_dir}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save splits: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a4bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize with configuration\n",
    "    splitter = DataSplitter(config_path=\"config/split_config.yaml\")\n",
    "    \n",
    "    # Execute splitting\n",
    "    X_train, X_test, y_train, y_test = splitter.split_data()\n",
    "    \n",
    "    # Log summary statistics\n",
    "    logger.info(\"\\n=== Split Summary ===\")\n",
    "    logger.info(f\"Training set: {len(X_train)} samples ({len(X_train)/len(X_train)+len(X_test)):.1%})\")\n",
    "    logger.info(f\"Test set: {len(X_test)} samples ({len(X_test)/len(X_train)+len(X_test)):.1%})\")\n",
    "    logger.info(f\"Target mean - Train: {y_train.mean():.2f}, Test: {y_test.mean():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
