{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab5dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (mean_squared_error, mean_absolute_error, \n",
    "                            r2_score, explained_variance_score)\n",
    "from sklearn.inspection import (permutation_importance, \n",
    "                              PartialDependenceDisplay)\n",
    "import shap\n",
    "from scipy import stats\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc15566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, models_path: Dict[str, str], test_data_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with model paths and test data\n",
    "        \n",
    "        Args:\n",
    "            models_path: Dictionary of model names and their file paths\n",
    "            test_data_path: Path to test data (CSV)\n",
    "        \"\"\"\n",
    "        self.models = self._load_models(models_path)\n",
    "        self.test_data = pd.read_csv(test_data_path)\n",
    "        self.X_test = self.test_data.drop('price', axis=1)\n",
    "        self.y_test = self.test_data['price']\n",
    "        self.results = {}\n",
    "        \n",
    "    def _load_models(self, model_paths: Dict[str, str]) -> Dict[str, object]:\n",
    "        \"\"\"Load pretrained models from pickle files\"\"\"\n",
    "        return {name: pickle.load(open(path, 'rb')) for name, path in model_paths.items()}\n",
    "    \n",
    "    def evaluate_all(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Run complete evaluation suite for all models\"\"\"\n",
    "        for name, model in self.models.items():\n",
    "            print(f\"\\nEvaluating {name}...\")\n",
    "            self.results[name] = {}\n",
    "            \n",
    "            # Basic metrics\n",
    "            self.results[name]['metrics'] = self._calculate_metrics(model)\n",
    "            \n",
    "            # Feature importance\n",
    "            self.results[name]['importance'] = self._feature_importance(model, name)\n",
    "            \n",
    "            # Inference speed\n",
    "            self.results[name]['inference_speed'] = self._measure_inference_speed(model)\n",
    "            \n",
    "            # Residual analysis\n",
    "            self.results[name]['residuals'] = self._analyze_residuals(model)\n",
    "            \n",
    "            # Statistical tests\n",
    "            if len(self.models) > 1:\n",
    "                self.results[name]['normality_test'] = self._check_residual_normality(\n",
    "                    self.results[name]['residuals']['residuals']\n",
    "                )\n",
    "        \n",
    "        # Comparative analysis\n",
    "        if len(self.models) > 1:\n",
    "            self._compare_models()\n",
    "            \n",
    "        return self.results\n",
    "    \n",
    "    def _calculate_metrics(self, model) -> Dict[str, float]:\n",
    "        \"\"\"Calculate evaluation metrics\"\"\"\n",
    "        y_pred = model.predict(self.X_test)\n",
    "        \n",
    "        return {\n",
    "            'RMSE': np.sqrt(mean_squared_error(self.y_test, y_pred)),\n",
    "            'MAE': mean_absolute_error(self.y_test, y_pred),\n",
    "            'R2': r2_score(self.y_test, y_pred),\n",
    "            'Explained Variance': explained_variance_score(self.y_test, y_pred),\n",
    "            'Mean Absolute Percentage Error': np.mean(np.abs((self.y_test - y_pred) / self.y_test)) * 100\n",
    "        }\n",
    "    \n",
    "    def _feature_importance(self, model, model_name: str) -> Dict:\n",
    "        \"\"\"Calculate feature importance using multiple methods\"\"\"\n",
    "        importance = {}\n",
    "        \n",
    "        # Permutation importance\n",
    "        perm_result = permutation_importance(\n",
    "            model, self.X_test, self.y_test, n_repeats=10, random_state=42\n",
    "        )\n",
    "        importance['permutation'] = {\n",
    "            'importances': perm_result.importances_mean,\n",
    "            'std': perm_result.importances_std\n",
    "        }\n",
    "        \n",
    "        # Model-specific importance\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance['native'] = model.feature_importances_\n",
    "            \n",
    "        # SHAP values (for tree-based models)\n",
    "        if model_name in ['Random Forest', 'Decision Tree', 'XGBoost']:\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            shap_values = explainer.shap_values(self.X_test)\n",
    "            importance['shap'] = np.abs(shap_values).mean(axis=0)\n",
    "        \n",
    "        return importance\n",
    "    \n",
    "    def _measure_inference_speed(self, model, n_iter: int = 100) -> Dict:\n",
    "        \"\"\"Measure inference latency\"\"\"\n",
    "        times = []\n",
    "        for _ in range(n_iter):\n",
    "            start = time.time()\n",
    "            model.predict(self.X_test.iloc[:1])  # Single prediction\n",
    "            times.append(time.time() - start)\n",
    "        \n",
    "        return {\n",
    "            'mean_latency_ms': np.mean(times) * 1000,\n",
    "            'p95_latency_ms': np.percentile(times, 95) * 1000\n",
    "        }\n",
    "    \n",
    "    def _analyze_residuals(self, model) -> Dict:\n",
    "        \"\"\"Analyze prediction residuals\"\"\"\n",
    "        y_pred = model.predict(self.X_test)\n",
    "        residuals = self.y_test - y_pred\n",
    "        \n",
    "        return {\n",
    "            'residuals': residuals,\n",
    "            'residual_mean': np.mean(residuals),\n",
    "            'residual_std': np.std(residuals),\n",
    "            'residual_skew': stats.skew(residuals)\n",
    "        }\n",
    "    \n",
    "    def _check_residual_normality(self, residuals) -> Dict:\n",
    "        \"\"\"Check if residuals follow normal distribution\"\"\"\n",
    "        stat, p = stats.shapiro(residuals)\n",
    "        return {\n",
    "            'shapiro_stat': stat,\n",
    "            'shapiro_p': p,\n",
    "            'is_normal': p > 0.05\n",
    "        }\n",
    "    \n",
    "    def _compare_models(self):\n",
    "        \"\"\"Compare models using statistical tests\"\"\"\n",
    "        model_names = list(self.models.keys())\n",
    "        for i in range(len(model_names)):\n",
    "            for j in range(i+1, len(model_names)):\n",
    "                m1 = model_names[i]\n",
    "                m2 = model_names[j]\n",
    "                \n",
    "                res1 = self.results[m1]['residuals']['residuals']\n",
    "                res2 = self.results[m2]['residuals']['residuals']\n",
    "                \n",
    "                # Paired t-test\n",
    "                t_stat, p_val = stats.ttest_rel(res1, res2)\n",
    "                self.results[f'{m1}_vs_{m2}'] = {\n",
    "                    't_test': {'statistic': t_stat, 'p_value': p_val},\n",
    "                    'better_model': m1 if np.mean(np.abs(res1)) < np.mean(np.abs(res2)) else m2\n",
    "                }\n",
    "    \n",
    "    def generate_report(self) -> None:\n",
    "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "        print(\"\\n=== MODEL EVALUATION REPORT ===\\n\")\n",
    "        \n",
    "        # Metrics comparison\n",
    "        metrics_df = pd.DataFrame({\n",
    "            name: data['metrics'] for name, data in self.results.items() \n",
    "            if 'metrics' in data\n",
    "        }).T\n",
    "        print(\"Performance Metrics:\")\n",
    "        print(metrics_df.sort_values('RMSE'))\n",
    "        \n",
    "        # Feature importance visualization\n",
    "        self._plot_feature_importance()\n",
    "        \n",
    "        # Residual analysis\n",
    "        self._plot_residuals()\n",
    "        \n",
    "        # Inference speed comparison\n",
    "        speed_df = pd.DataFrame({\n",
    "            name: data['inference_speed'] for name, data in self.results.items()\n",
    "            if 'inference_speed' in data\n",
    "        }).T\n",
    "        print(\"\\nInference Speed (ms):\")\n",
    "        print(speed_df.sort_values('mean_latency_ms'))\n",
    "    \n",
    "    def _plot_feature_importance(self) -> None:\n",
    "        \"\"\"Visualize feature importance across models\"\"\"\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        for i, (name, data) in enumerate(self.results.items()):\n",
    "            if 'importance' not in data:\n",
    "                continue\n",
    "                \n",
    "            plt.subplot(2, 2, i+1)\n",
    "            if 'shap' in data['importance']:\n",
    "                importances = data['importance']['shap']\n",
    "            elif 'native' in data['importance']:\n",
    "                importances = data['importance']['native']\n",
    "            else:\n",
    "                importances = data['importance']['permutation']['importances']\n",
    "                \n",
    "            sorted_idx = np.argsort(importances)\n",
    "            plt.barh(self.X_test.columns[sorted_idx], importances[sorted_idx])\n",
    "            plt.title(f\"{name} Feature Importance\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_residuals(self) -> None:\n",
    "        \"\"\"Plot residual distributions\"\"\"\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i, (name, data) in enumerate(self.results.items()):\n",
    "            if 'residuals' not in data:\n",
    "                continue\n",
    "                \n",
    "            plt.subplot(1, len(self.models), i+1)\n",
    "            sns.histplot(data['residuals']['residuals'], kde=True)\n",
    "            plt.title(f\"{name} Residuals\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04890dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    MODEL_PATHS = {\n",
    "        \"Decision Tree\": \"decision_tree_model.pkl\",\n",
    "        \"Random Forest\": \"random_forest_model.pkl\",\n",
    "        \"XGBoost\": \"xgboost_model.pkl\",\n",
    "        \"Ridge\": \"ridge_model.pkl\"\n",
    "    }\n",
    "    TEST_DATA_PATH = \"test_data.csv\"\n",
    "    \n",
    "    # Initialize and run evaluation\n",
    "    evaluator = ModelEvaluator(MODEL_PATHS, TEST_DATA_PATH)\n",
    "    results = evaluator.evaluate_all()\n",
    "    \n",
    "    # Generate report\n",
    "    evaluator.generate_report()\n",
    "    \n",
    "    # Save full results\n",
    "    with open('evaluation_results.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sickit-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
