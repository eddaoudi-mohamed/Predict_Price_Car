{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97348696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê Scraping page 2...\n",
      "https://www.avito.ma/fr/maroc/voitures?o=2\n",
      "[+] Found 34 cars on page 2\n",
      "‚úÖ Saved 34 cars from page 2\n",
      "\n",
      "üåê Scraping page 3...\n",
      "https://www.avito.ma/fr/maroc/voitures?o=3\n",
      "[+] Found 35 cars on page 3\n",
      "[!] Retry 1 - Status: 511\n",
      "[!] Retry 1 - Status: 511\n",
      "‚úÖ Saved 35 cars from page 3\n",
      "\n",
      "üåê Scraping page 4...\n",
      "https://www.avito.ma/fr/maroc/voitures?o=4\n",
      "[+] Found 34 cars on page 4\n",
      "‚úÖ Saved 34 cars from page 4\n",
      "\n",
      "üåê Scraping page 5...\n",
      "https://www.avito.ma/fr/maroc/voitures?o=5\n",
      "[+] Found 34 cars on page 5\n",
      "[!] Retry 1 - Status: 511\n",
      "‚úÖ Saved 34 cars from page 5\n",
      "\n",
      "üéâ Done scraping all pages!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ----------------------------------------\n",
    "# Config\n",
    "# ----------------------------------------\n",
    "\n",
    "base_url = 'https://www.avito.ma/fr/maroc/voitures'\n",
    "output_file = 'avito_cars.csv'\n",
    "max_pages = 2609\n",
    "max_threads = 10\n",
    "retry_limit = 3\n",
    "\n",
    "# ----------------------------------------\n",
    "# Headers Rotation\n",
    "# ----------------------------------------\n",
    "\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64)...',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)...',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64)...',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X)...',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64)... Safari/537.36',\n",
    "]\n",
    "\n",
    "def get_headers():\n",
    "    return {\n",
    "        'User-Agent': random.choice(user_agents)\n",
    "    }\n",
    "\n",
    "# ----------------------------------------\n",
    "# CSV Setup\n",
    "# ----------------------------------------\n",
    "\n",
    "if not os.path.exists(output_file):\n",
    "    pd.DataFrame(columns=[\n",
    "        'url', 'type_boit', 'type_carburant',\n",
    "        'kilometrage', 'marke', 'model',\n",
    "        'puissance', 'price' , \n",
    "        'has_abs' ,'has_airbags' , 'has_bluetooth_audio' ,'has_rear_camera' , 'has_ac' , 'has_esp'\n",
    "          , 'has_alloy_wheels' , 'has_speed_limiter' , 'has_trip_computer' ,'has_parking_sensor' , \n",
    "          'has_cruise_control' , 'has_leather_seats' , 'has_gps',  'has_sunroof' , 'has_remote_locking'\n",
    "          ,'has_power_windows' \n",
    "    ]).to_csv(output_file, index=False)\n",
    "\n",
    "# ----------------------------------------\n",
    "# Safe Request with Retry\n",
    "# ----------------------------------------\n",
    "\n",
    "def fetch_with_retries(url, params=None):\n",
    "    for attempt in range(retry_limit):\n",
    "        try:\n",
    "            response = requests.get(url, headers=get_headers(), params=params, timeout=10)\n",
    "            if response.status_code == 200 and \"captcha\" not in response.text.lower():\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"[!] Retry {attempt+1} - Status: {response.status_code}\")\n",
    "                time.sleep(random.uniform(2, 5))\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Error fetching {url}: {e}\")\n",
    "            time.sleep(random.uniform(2, 4))\n",
    "    return None\n",
    "\n",
    "# ----------------------------------------\n",
    "# Car Detail Scraper\n",
    "# ----------------------------------------\n",
    "def scrape_car(url):\n",
    "    try:\n",
    "        if not url.startswith(\"http\"):\n",
    "            url = \"https://www.avito.ma\" + url\n",
    "\n",
    "        response = fetch_with_retries(url)\n",
    "        if not response:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # --- PRICE ---\n",
    "        price_tag = soup.find('p', {'class': 'jdRkSM'})\n",
    "        price = price_tag.text.strip() if price_tag else None\n",
    "\n",
    "        # --- BASIC FEATURES (first div.dnArJl) ---\n",
    "        features_divs = soup.find_all('div', {'class': 'dnArJl'})\n",
    "        values = labels = []\n",
    "        if len(features_divs) >= 1:\n",
    "            values = features_divs[0].find_all('span', {'class': 'fjZBup'})\n",
    "            labels = [s.text.strip() for s in features_divs[0].find_all('span', {'class': 'bXFCIH'})]\n",
    "\n",
    "        get = lambda i: values[i].text.strip() if len(values) > i else None\n",
    "        get_by_label = lambda name: get(labels.index(name)) if name in labels else None\n",
    "\n",
    "        # --- Feature Mapping (renamed columns) ---\n",
    "\n",
    "        real_features = {\n",
    "            'ABS': 'has_abs',\n",
    "            'Airbags': 'has_airbags',\n",
    "            'CD/MP3/Bluetooth': 'has_bluetooth_audio',\n",
    "            'Cam√©ra de recul': 'has_rear_camera',\n",
    "            'Climatisation': 'has_ac',\n",
    "            'ESP': 'has_esp',\n",
    "            'Jantes aluminium': 'has_alloy_wheels',\n",
    "            'Limiteur de vitesse': 'has_speed_limiter',\n",
    "            'Ordinateur de bord': 'has_trip_computer',\n",
    "            'Radar de recul': 'has_parking_sensor',\n",
    "            'R√©gulateur de vitesse': 'has_cruise_control',\n",
    "            'Si√®ges cuir': 'has_leather_seats',\n",
    "            'Syst√®me de navigation/GPS': 'has_gps',\n",
    "            'Toit ouvrant': 'has_sunroof',\n",
    "            'Verrouillage centralis√© √† distance': 'has_remote_locking',\n",
    "            'Vitres √©lectriques': 'has_power_windows'\n",
    "        }\n",
    "\n",
    "        # --- EXTRA FEATURES (second div.dnArJl) ---\n",
    "        feature_status = {new_key: 0 for new_key in real_features.values()}  # default to 0\n",
    "        if len(features_divs) >= 2:\n",
    "            feature_texts = [\n",
    "                span.text.strip() for span in features_divs[1].find_all('span', {'class': 'fjZBup'})\n",
    "            ]\n",
    "            feature_status = {\n",
    "                new_key: 1 if old_key in feature_texts else 0\n",
    "                for old_key, new_key in real_features.items()\n",
    "            }\n",
    "        # --- Final Data Dictionary ---\n",
    "      \n",
    "        return {\n",
    "            'url': url,\n",
    "            'type_boit': get(1),\n",
    "            'type_carburant': get(2),\n",
    "            'kilometrage': get(3),\n",
    "            'marke': get(4),\n",
    "            'model': get(5),\n",
    "            'puissance': get_by_label('Puissance fiscale'),\n",
    "            'price': price,\n",
    "            **feature_status  # Merge feature flags into main dictionary\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error scraping car {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "for page in range(2, 6):\n",
    "    print(f\"\\nüåê Scraping page {page}...\")\n",
    "    params = {'o': page}\n",
    "    car_links = []\n",
    "\n",
    "    response = fetch_with_retries(base_url, params=params)\n",
    "    print(response.url)\n",
    "    if response:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        container = soup.find('div', {'class': 'crKvIr'})\n",
    "        if container:\n",
    "            links = container.find_all('a', {'class': 'jZXrfL'})\n",
    "            car_links = [link['href'] for link in links if link.get('href')]\n",
    "            print(f\"[+] Found {len(car_links)} cars on page {page}\")\n",
    "        else:\n",
    "            print(\"[!] No container found on this page\")\n",
    "    else:\n",
    "        print(f\"[!] Failed to fetch page {page}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    page_data = []\n",
    "    with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "        futures = [executor.submit(scrape_car, url) for url in car_links]\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                page_data.append(result)\n",
    "\n",
    "    # Save after each page\n",
    "    if page_data:\n",
    "        pd.DataFrame(page_data).to_csv(output_file, mode='a', header=False, index=False)\n",
    "        print(f\"‚úÖ Saved {len(page_data)} cars from page {page}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No data to save for page {page}\")\n",
    "\n",
    "    # Random polite delay\n",
    "    if page % 20 == 0:\n",
    "        print(\"üò¥ Taking longer break...\")\n",
    "        time.sleep(random.uniform(10, 20))\n",
    "    else:\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "\n",
    "print(\"\\nüéâ Done scraping all pages!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
