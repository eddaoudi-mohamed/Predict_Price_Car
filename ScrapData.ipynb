{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97348696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ----------------------------------------\n",
    "# Config\n",
    "# ----------------------------------------\n",
    "\n",
    "base_url = 'https://www.avito.ma/fr/maroc/voitures'\n",
    "output_file = 'avito_cars_data.csv'\n",
    "max_pages = 2609\n",
    "max_threads = 10\n",
    "retry_limit = 3\n",
    "\n",
    "# ----------------------------------------\n",
    "# Headers Rotation\n",
    "# ----------------------------------------\n",
    "\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64)...',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)...',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64)...',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X)...',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64)... Safari/537.36',\n",
    "]\n",
    "\n",
    "def get_headers():\n",
    "    return {\n",
    "        'User-Agent': random.choice(user_agents)\n",
    "    }\n",
    "\n",
    "# ----------------------------------------\n",
    "# CSV Setup\n",
    "# ----------------------------------------\n",
    "\n",
    "if not os.path.exists(output_file):\n",
    "    pd.DataFrame(columns=[\n",
    "        'url', 'type_boit', 'type_carburant',\n",
    "        'kilometrage', 'marke', 'model',\n",
    "        'puissance', 'price'\n",
    "    ]).to_csv(output_file, index=False)\n",
    "\n",
    "# ----------------------------------------\n",
    "# Safe Request with Retry\n",
    "# ----------------------------------------\n",
    "\n",
    "def fetch_with_retries(url, params=None):\n",
    "    for attempt in range(retry_limit):\n",
    "        try:\n",
    "            response = requests.get(url, headers=get_headers(), params=params, timeout=10)\n",
    "            if response.status_code == 200 and \"captcha\" not in response.text.lower():\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"[!] Retry {attempt+1} - Status: {response.status_code}\")\n",
    "                time.sleep(random.uniform(2, 5))\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Error fetching {url}: {e}\")\n",
    "            time.sleep(random.uniform(2, 4))\n",
    "    return None\n",
    "\n",
    "# ----------------------------------------\n",
    "# Car Detail Scraper\n",
    "# ----------------------------------------\n",
    "\n",
    "def scrape_car(url):\n",
    "    try:\n",
    "        if not url.startswith(\"http\"):\n",
    "            url = \"https://www.avito.ma\" + url\n",
    "\n",
    "        response = fetch_with_retries(url)\n",
    "        if not response:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        price_tag = soup.find('p', {'class': 'jdRkSM'})\n",
    "        price = price_tag.text.strip() if price_tag else None\n",
    "\n",
    "        features_div = soup.find('div', {'class': 'dnArJl'})\n",
    "        if features_div:\n",
    "            values = features_div.find_all('span', {'class': 'fjZBup'})\n",
    "            labels = [s.text.strip() for s in features_div.find_all('span', {'class': 'bXFCIH'})]\n",
    "            get = lambda i: values[i].text.strip() if len(values) > i else None\n",
    "            get_by_label = lambda name: get(labels.index(name)) if name in labels else None\n",
    "\n",
    "            return {\n",
    "                'url': url,\n",
    "                'type_boit': get(1),\n",
    "                'type_carburant': get(2),\n",
    "                'kilometrage': get(3),\n",
    "                'marke': get(4),\n",
    "                'model': get(5),\n",
    "                'puissance': get_by_label('Puissance fiscale'),\n",
    "                'price': price\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'url': url,\n",
    "                'type_boit': None,\n",
    "                'type_carburant': None,\n",
    "                'kilometrage': None,\n",
    "                'marke': None,\n",
    "                'model': None,\n",
    "                'puissance': None,\n",
    "                'price': price\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error scraping car {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# ----------------------------------------\n",
    "# Main Scraping Loop\n",
    "# ----------------------------------------\n",
    "\n",
    "for page in range(1358, max_pages + 1):\n",
    "    print(f\"\\nüåê Scraping page {page}...\")\n",
    "    params = {'p': page, 'radius': 200}\n",
    "    car_links = []\n",
    "\n",
    "    response = fetch_with_retries(base_url, params=params)\n",
    "    if response:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        container = soup.find('div', {'class': 'crKvIr'})\n",
    "        if container:\n",
    "            links = container.find_all('a', {'class': 'jZXrfL'})\n",
    "            car_links = [link['href'] for link in links if link.get('href')]\n",
    "            print(f\"[+] Found {len(car_links)} cars on page {page}\")\n",
    "        else:\n",
    "            print(\"[!] No container found on this page\")\n",
    "    else:\n",
    "        print(f\"[!] Failed to fetch page {page}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Scrape detail pages in parallel\n",
    "    page_data = []\n",
    "    with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "        futures = [executor.submit(scrape_car, url) for url in car_links]\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                page_data.append(result)\n",
    "\n",
    "    # Save after each page\n",
    "    if page_data:\n",
    "        pd.DataFrame(page_data).to_csv(output_file, mode='a', header=False, index=False)\n",
    "        print(f\"‚úÖ Saved {len(page_data)} cars from page {page}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No data to save for page {page}\")\n",
    "\n",
    "    # Random polite delay\n",
    "    if page % 20 == 0:\n",
    "        print(\"üò¥ Taking longer break...\")\n",
    "        time.sleep(random.uniform(10, 20))\n",
    "    else:\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "\n",
    "print(\"\\nüéâ Done scraping all pages!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
